{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Car Speed from Dashcam Video\n",
    "\n",
    "My approach will be to calculate optical flow between pairs of successive frames and use that as a metric to predict the car speed via a CNN. \n",
    "\n",
    "A general outline of what we'll be doing:\n",
    "\n",
    "1. Split up given data into training & test\n",
    "2. Preprocess frames\n",
    "    * Augment training frames with regularized illumination factor\n",
    "    * Crop noise out of the frame (car dashboard, etc.) \n",
    "    * Interpolate image to input size of neural network\n",
    "3. Train, validate, hyperparameterize CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflow import pyflow\n",
    "import skvideo.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "\n",
    "VIDEO_FP = \"/Users/shivam/dev/comma/data/train.mp4\"\n",
    "TEST_VIDEO_FP = \"/Users/shivam/dev/comma/data/test.mp4\"\n",
    "SPEEDS_FP = \"/Users/shivam/dev/comma/data/train.txt\"\n",
    "NUM_FRAMES = 20400\n",
    "\n",
    "# Folder names\n",
    "MAIN = \"main\"\n",
    "VALIDATION = \"validation\"\n",
    "EVALUATION = \"evaluation\"\n",
    "TRAIN = \"train\"\n",
    "\n",
    "# Model input size\n",
    "N_img_height = 66\n",
    "N_img_width = 220\n",
    "N_img_channels = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Construct Dataset\n",
    "\n",
    "Let's split the video into frames & save them as images in a seperate folder. \n",
    "\n",
    "This way we don't need to have the entire video in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_folder(target_folder):\n",
    "    if os.path.exists(target_folder):\n",
    "        shutil.rmtree(target_folder)\n",
    "        \n",
    "    os.mkdir(target_folder)\n",
    "    \n",
    "def get_image_path(folder, num_frame):\n",
    "    return os.path.join(folder, str(num_frame) + '.jpg')\n",
    "\n",
    "def load_image(image_path):\n",
    "    bgr_img = cv2.imread(image_path)\n",
    "    rbg_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "    return rbg_img\n",
    "\n",
    "def construct_dataset(video_fp, target_folder, num_frames):\n",
    "    \n",
    "    create_empty_folder(target_folder)\n",
    "    \n",
    "    video_frames = skvideo.io.vreader(video_fp)\n",
    "    \n",
    "    for idx, frame in enumerate(tqdm_notebook(video_frames, total=num_frames)):\n",
    "        img_path = get_image_path(target_folder, idx)\n",
    "        skvideo.io.vwrite(img_path, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a0dbb3d5784563a648fbca02e526c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "construct_dataset(VIDEO_FP, MAIN, NUM_FRAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also read in the speed data.\n",
    "\n",
    "Since we atleast need two frames to determine speed, we won't consider the speed of the first frame a valid training point since it has no prior frame. As such, we'll drop it from the dataframe. We will use the average speed between two successive frames as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speeds_to_df(speeds_fp):\n",
    "    speeds_df = pd.read_csv(speeds_fp, header=None)\n",
    "    speeds_df.columns = ['speed']\n",
    "    speeds_df['speed'] = speeds_df['speed'].rolling(2).mean()\n",
    "    speeds_df.drop(speeds_df.index[0], inplace=True) \n",
    "    return speeds_df\n",
    "\n",
    "speeds_df = speeds_to_df(SPEEDS_FP)\n",
    "speeds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(speeds_df)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('speed')\n",
    "plt.title('Car Speed - Train Video')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping\n",
    "\n",
    "Let's take a look at what we can remove from the image (sky, dashboard, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_crop(image):\n",
    "    cropped_image = image[80:350, ...]\n",
    "    return cropped_image\n",
    "\n",
    "crop_dummy = load_image(get_image_path(MAIN, 1400))\n",
    "plt.imshow(crop_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(custom_crop(crop_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Optical Flow\n",
    "\n",
    "I want to try calculating dense optical flow in two methods:\n",
    "\n",
    "1. Farneback Method\n",
    "2. C. Liu. Method detailed in \n",
    "\n",
    "C. Liu. Beyond Pixels: Exploring New Representations and Applications for Motion Analysis. Doctoral Thesis. Massachusetts Institute of Technology. May 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_liu_optical_flow(img_1, img_2):\n",
    "    alpha = 0.012\n",
    "    ratio = 0.75\n",
    "    minWidth = 20\n",
    "    nOuterFPIterations = 7\n",
    "    nInnerFPIterations = 1\n",
    "    nSORIterations = 10\n",
    "    colType = 0\n",
    "    \n",
    "    im_1 = np.array(img_1).astype(float) / 255.\n",
    "    im_2 = np.array(img_2).astype(float) / 255.\n",
    "    \n",
    "    u, v, im2W = pyflow.coarse2fine_flow(\n",
    "        im_1, im_2, alpha, ratio, minWidth, nOuterFPIterations, nInnerFPIterations,\n",
    "        nSORIterations, colType)\n",
    "    \n",
    "    hsv = np.zeros(img_2.shape, dtype=np.uint8)\n",
    "\n",
    "    # take saturation values from second image\n",
    "    #hsv[:, :, 1] = cv2.cvtColor(img_2, cv2.COLOR_RGB2HSV)[:,:,1]\n",
    "    hsv[:, :, 1] = 255\n",
    "    \n",
    "    mag, ang = cv2.cartToPolar(u, v)\n",
    "\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return rgb\n",
    "\n",
    "def calculate_farneback_optical_flow(image_current, image_next):\n",
    "    gray_current = cv2.cvtColor(image_current, cv2.COLOR_RGB2GRAY)\n",
    "    gray_next = cv2.cvtColor(image_next, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    \n",
    "    hsv = np.zeros(image_next.shape, dtype=np.uint8)\n",
    "    # set saturation\n",
    "    hsv[:,:,1] = cv2.cvtColor(image_next, cv2.COLOR_RGB2HSV)[:,:,1]\n",
    " \n",
    "    # Flow Parameters\n",
    "    flow_mat = None\n",
    "    image_scale = 0.5\n",
    "    nb_images = 1\n",
    "    win_size = 15\n",
    "    nb_iterations = 2\n",
    "    deg_expansion = 5\n",
    "    STD = 1.3\n",
    "    extra = 0\n",
    "\n",
    "    # obtain dense optical flow paramters\n",
    "    flow = cv2.calcOpticalFlowFarneback(gray_current, gray_next,  \n",
    "                                        flow_mat, \n",
    "                                        image_scale, \n",
    "                                        nb_images, \n",
    "                                        win_size, \n",
    "                                        nb_iterations, \n",
    "                                        deg_expansion, \n",
    "                                        STD, \n",
    "                                        0)\n",
    "                                        \n",
    "        \n",
    "    # convert from cartesian to polar\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])  \n",
    "        \n",
    "    # hue corresponds to direction\n",
    "    hsv[...,0] = ang * (180/ np.pi / 2)\n",
    "    \n",
    "    # value corresponds to magnitude\n",
    "    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    # convert HSV to int32's\n",
    "    #hsv = np.asarray(hsv, dtype= np.float32)\n",
    "    rgb_flow = cv2.cvtColor(hsv,cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return rgb_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = load_image(get_image_path(MAIN, 1400))\n",
    "B = load_image(get_image_path(MAIN, 1401))\n",
    "plt.imshow(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = calculate_liu_optical_flow(A, B)\n",
    "plt.imshow(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = calculate_farneback_optical_flow(A, B)\n",
    "plt.imshow(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The farneback method is much faster. On top of that, it looks like it's better at picking up flow vectors from static markers. These flow vectors are more important than those from noise like cars, etc. as there are more variables affecting the flow vectors of other moving objects than static markers. In fact, if we wanted to do better, we should find a way of identifying flow vectors from static objects & give more weight to those. \n",
    "\n",
    "We should also take some more knowledge of the real world and apply as that as a heuristic to filter noise. For instance, if the camera moves in space towards any point in the field of view then the flow vectors from static objects should all intersect at that point. \n",
    "\n",
    "A visual example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(D)\n",
    "plt.arrow(310,225, -270, -60, color=\"red\", head_width=20, head_length=20)\n",
    "plt.arrow(310,225, 150, 80, color=\"red\", head_width=20, head_length=20)\n",
    "plt.arrow(310,225, -250, 170, color=\"red\", head_width=20, head_length=20)\n",
    "plt.arrow(310,225, 300, -50, color=\"red\", head_width=20, head_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect this kind of analysis to remove the vectors from the cars in the image. However, we don't always have this intersection in our field of view. Sometimes the car can be turning fast & the camera moves towards a point it does not have in its FOV. So, we would apply this heuristic with the appropriate precautions.\n",
    "\n",
    "Before going deep into this, I'd like to have a baseline model & then apply this kind of analysis if it performs poorly. Let's start by creating our test & train data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data -> Train / Validation / Evaluation\n",
    "\n",
    "From the data for which we know the speeds, we will split it into training, validation, and evaluation data.\n",
    "\n",
    "We will train the model on training, cross validate & tune with validation, and then finally evaluate performance on the evaluation set. \n",
    "\n",
    "We can randomly shuffle the available training frames into these three sets, but I don't think that's good enough. It's quite likely that most of the evaluation and validation sets will not be contigious. This means the model will have already seen small bits and pieces of very similar environments. I want to set aside about 30% of the validation & evaluation sets as continious frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df, split):\n",
    "    continuous_train_df , continuous_test_df = train_test_split(df, test_size=0.05, shuffle=False, random_state=46)\n",
    "    train_df, shuffle_test_df = train_test_split(continuous_train_df, test_size=split, random_state=46)\n",
    "    \n",
    "    test_df = pd.concat([shuffle_test_df, continuous_test_df])\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, evaluation_df = split_dataset(speeds_df, 0.10)\n",
    "train_df, validation_df = split_dataset(train_df, 0.20)\n",
    "\n",
    "# Sanity Check; this should give us the number of training points available\n",
    "print(train_df.shape[0] + evaluation_df.shape[0] + validation_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can process pairs of successive frames to build the train, validation, and evaluation sets. \n",
    "We will crop the frames, calculate optical flow, and resize the result to the input size of our neural network. \n",
    "The one difference that sets apart the train set is that pairs of images should be augmented by an illumination factor to regularize against brightness changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_resize(image):\n",
    "    resized_image = cv2.resize(image, (N_img_width, N_img_height), interpolation = cv2.INTER_AREA)\n",
    "    return resized_image\n",
    "\n",
    "def change_brightness(image, bright_factor):\n",
    "    \"\"\"\n",
    "    Augments the brightness of the image\n",
    "    Input: image (RGB)\n",
    "    returns: image with brightness augmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    # perform brightness augmentation only on the second channel\n",
    "    hsv_image[:,:,2] = hsv_image[:,:,2] * bright_factor\n",
    "    \n",
    "    # change back to RGB\n",
    "    image_rgb = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "    return image_rgb\n",
    "\n",
    "def construct_optical_flow_dataset(frames_df, dataset_folder, target_folder, augment_brightness=False):\n",
    "    \n",
    "    create_empty_folder(target_folder)\n",
    "    \n",
    "    for idx, row in tqdm_notebook(frames_df.iterrows(), total=frames_df.shape[0]):\n",
    "        curr_img_path = get_image_path(dataset_folder, idx)\n",
    "        preceding_img_path = get_image_path(dataset_folder, idx - 1)\n",
    "        \n",
    "        curr_img = load_image(curr_img_path)\n",
    "        preceding_img = load_image(preceding_img_path)\n",
    "        \n",
    "        curr_img = custom_crop(curr_img)\n",
    "        preceding_img = custom_crop(preceding_img)\n",
    "        \n",
    "        if augment_brightness:\n",
    "            bright_factor = np.random.uniform(0,1.3)\n",
    "            curr_img = change_brightness(curr_img, bright_factor)\n",
    "            preceding_img = change_brightness(preceding_img, bright_factor)\n",
    "            \n",
    "        flow = calculate_farneback_optical_flow(preceding_img, curr_img)\n",
    "        \n",
    "        flow = custom_resize(flow)\n",
    "        \n",
    "        curr_flow_path = os.path.join(target_folder, str(idx) + '.npy')\n",
    "        np.save(curr_flow_path, flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_optical_flow_dataset(train_df, MAIN, TRAIN, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_optical_flow_dataset(validation_df, MAIN, VALIDATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_optical_flow_dataset(evaluation_df, MAIN, EVALUATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much faster to precompute the flow values and then use the generator below to feed training & validation samples to the neural net. The reason for this is because the network will reuse the same images many times, resulting in unneccessary recomputation of flow values. It's best to precompute all these, rather than have them in memory or generate them on the fly each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(dataset_folder, df, batch_size=16):\n",
    "    input_batch = np.zeros((batch_size, N_img_height, N_img_width, 3))\n",
    "    output_batch = np.zeros((batch_size))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            data_idx = np.random.randint(0, len(df) - 1)\n",
    "            row = df.iloc[data_idx]\n",
    "            flow_path = os.path.join(dataset_folder, str(row.name) + '.npy')\n",
    "            flow = np.load(flow_path)\n",
    "            input_batch[i] = flow\n",
    "            output_batch[i] = row['speed']\n",
    "    \n",
    "        yield (input_batch, output_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.core import Activation, Dropout, Flatten, Dense, Lambda\n",
    "from keras.layers import ELU\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nvidia_model():\n",
    "    inputShape = (N_img_height, N_img_width, N_img_channels)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Lambda(lambda x: x / 127.5 - 1, input_shape = inputShape))\n",
    "\n",
    "    model.add(Convolution2D(24, (5, 5), \n",
    "                            strides=(2,2), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv1'))\n",
    "    \n",
    "    \n",
    "    model.add(ELU())    \n",
    "    model.add(Convolution2D(36, (5, 5), \n",
    "                            strides=(2,2), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv2'))\n",
    "    \n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(48, (5, 5), \n",
    "                            strides=(2,2), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv3'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(64, (3, 3), \n",
    "                            strides = (1,1), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv4'))\n",
    "    \n",
    "    model.add(ELU())              \n",
    "    model.add(Convolution2D(64, (3, 3), \n",
    "                            strides= (1,1), \n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            name = 'conv5'))\n",
    "              \n",
    "              \n",
    "    model.add(Flatten(name = 'flatten'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(100, kernel_initializer = 'he_normal', name = 'fc1'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(50, kernel_initializer = 'he_normal', name = 'fc2'))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(10, kernel_initializer = 'he_normal', name = 'fc3'))\n",
    "    model.add(ELU())\n",
    "    \n",
    "    model.add(Dense(1, name = 'output', kernel_initializer = 'he_normal'))\n",
    "    \n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer = adam, loss = 'mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "filepath = 'model-weights-Vtest3.h5'\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience=1, \n",
    "                              verbose=1, \n",
    "                              min_delta = 0.23,\n",
    "                              mode='min',)\n",
    "\n",
    "modelCheckpoint = ModelCheckpoint(filepath, \n",
    "                                  monitor = 'val_loss', \n",
    "                                  save_best_only = True, \n",
    "                                  mode = 'min', \n",
    "                                  verbose = 1,\n",
    "                                 save_weights_only = True)\n",
    "callbacks_list = [modelCheckpoint, earlyStopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generate_samples(TRAIN, train_df)\n",
    "validation_generator = generate_samples(VALIDATION, validation_df, 1)\n",
    "model = nvidia_model()\n",
    "history = model.fit_generator(\n",
    "        train_generator, \n",
    "        steps_per_epoch = 400, \n",
    "        epochs = 25,\n",
    "    callbacks = callbacks_list,\n",
    "        verbose = 1,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = len(validation_df) - 1)\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = generate_samples(VALIDATION, validation_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.iloc[0].name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
